# 关于过拟合和解决办法

## 1、什么是过拟合、欠拟合

过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，**模型在训练集上表现很好，但在测试集上却表现很差**。模型对训练集"死记硬背"（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，**泛化能力差**。

**为什么会出现过拟合现象？**

造成原因主要有以下几种：
1、**训练数据集样本单一，样本不足**。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。
2、**训练数据中噪声干扰过大**。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。
3、**模型过于复杂。**模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。



有过拟合就有欠拟合。

欠拟合是指模型不能在训练集上获得足够低的误差。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。

**如何解决欠拟合？**

欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过**增加网络复杂度**或者在模型中**增加特征**，这些都是很好解决欠拟合的方法。



## 2、如何缓解过拟合

**2.1 数据增强**

让机器学习或深度学习模型泛化能力更好的办法就是使用更多的数据进行训练。但是，在实践中，我们拥有的数据量是有限的。解决这个问题的一种方法就是**创建“假数据”并添加到训练集中——数据集增强**。通过增加训练集的额外副本来增加训练集的大小，进而改进模型的泛化能力。

我们以图像数据集举例，能够做：旋转图像、缩放图像、随机裁剪、加入随机噪声、平移、镜像等方式来增加数据量。另外补充一句，在物体分类问题里，**CNN在图像识别的过程中有强大的“不变性”规则，即待辨识的物体在图像中的形状、姿势、位置、图像整体明暗度都不会影响分类结果**。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充。

【TODO 补充NLP数据增强方法】

**2.2 采用合适的模型（控制模型的复杂度）**

过于复杂的模型会带来过拟合问题。对于模型的设计，目前公认的一个深度学习规律"deeper is better"。国内外各种大牛通过实验和竞赛发现，对于CNN来说，层数越多效果越好，但是也更容易产生过拟合，并且计算所耗费的时间也越长。

根据**奥卡姆剃刀法则**：在同样能够解释已知观测现象的假设中，我们应该挑选“最简单”的那一个。对于模型的设计而言，我们应该**选择简单、合适的模型解决复杂的问题**。

**2.3 降低特征的数量**

对于一些**特征工程**而言，可以降低特征的数量——删除冗余特征，人工选择保留哪些特征。这种方法也可以解决过拟合问题。

**2.4 正则化方法**

正则化（Regularization）是机器学习中一种常用的技术，其主要目的是控制模型复杂度，减小过拟合。最基本的正则化方法是在原目标（代价）函数 中添加惩罚项，对复杂度高的模型进行“惩罚”。其数学表达形式为：

$\widetilde J (w;X,y) = J(w;X,y)+\lambda \Omega (w)$。

式中 $X$ 、 $y$为训练样本和相应标签，$ w $为权重系数向量；$ J() $为目标函数， $Ω(w)$ 即为惩罚项，可理解为模型“规模”的某种度量；参数$\lambda$ 控制控制正则化强弱。不同的$ Ω$ 函数对权重 $w$ 的最优解有不同的偏好，因而会产生不同的正则化效果。

如果我们使用梯度下降法的话，$w$的更新变为:
$$
w \rightarrow w^{\prime} = w - ( \alpha \frac{\partial J}{\partial w} + \lambda \frac{\partial \Omega}{\partial w} )
$$
我们看到参数更新比没有加入正则化的情况下，会多减去(或加上)一个“惩罚项”。我们下面根据具体的正则化项具体分析。

1. L1正则化(Lasso regularizer)

   $\lambda \Omega (w) = $


1. L2正则化

**2.5 Dropout**

**2.6  early stopping（提前终止）**

**2.7 正规化方法**

**2.8 集成学习**















