# 线性回归知识

## 1、问题描述
线性回归(Linear regression)是利用**回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间**关系进行建模的一种分析方式。

那么他们之间的关系可以表示为： 

$
y = f(x|w_1, w_2, ... , w_n, b) = w_1*x_1 + w_2*x_2 + ... + w_n*x_n + b
$

再写成向量形式的话：

$
y = f(x|w, b) = w^Tx+b
$

这里的 $w = (w_1; w_2; ...; w_n)$, $x = (x_1; x_2; ...; x_n)$。

注意这里的$w$和$x$均是列向量。

**我们的任务是去确定(学习)到$w$和$b$。**

那么怎么理解呢？我们来看几个例子

- 期末成绩：0.7×考试成绩+0.3×平时成绩
- 房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率

**注意：线性、非线性、回归的区别**

- **线性：**两个变量之间的关系**是一次函数关系**的——图象**是直线**，叫做线性。

  > **题目的线性是指广义的线性，也就是数据与数据之间的关系。**
  > **非线性：**两个变量之间的关系**不是**一次函数关系**的——图象**不是直线，叫做非线性。

- **回归：**人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算**回归到真实值**，这就是回归的由来。

## 2、损失函数

我们在<a href="/#/machine-learning/loss_func/common_loss_func.md" target="_blank">常见的损失函数</a>中详细介绍了一些常见的损失函数。
线性回归问题中，我们常用的一个损失函数是均方误差(Mean Square Error, MSE)

$$
loss(w, b) = \frac1n \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$
写成向量形式为:
$$
loss(W, b)  = (Y - \hat{Y})^2
$$

这里的$\hat{y}$和$\hat{Y}$是模型的预测值。

损失函数衡量的是真实值与预测值的差异，那我们的问题就可以转换为： 学习到$w$和$b$使得损失函数$MSE$的值最小的最优化问题， 即：

$$
\begin{aligned}
&\ \arg\min_{w, b} loss(w, b)  \\
&= \arg\min_{w, b} \frac1n \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 \\
&= \arg\min_{w, b} \frac1n \sum_{i=1}^{n}[y_i - (wx_i + b)]^2
\end{aligned}
$$

写成向量形式为:
$$
\arg\min_{W, b} [Y - (W^TX+b)]^2
$$


## 3、最小二乘法

我在《数值分析》和《数理统计》这两门课中都学到了这部分内容。《数理统计》好像称为最小二乘估计。

线性回归里面用到的最小二乘法指的是线性最小二乘，当然就有非线性最小二乘法了，我们在后面视情况表出。

进入正题：

为了使得我们的损失函数更加简洁，我们可以假设有一个特征$x_0$恒为$1$，那么我们再使用一个$w_0*x_0$去替代损失函数中的$b$,那么我们的损失函数就变成了
$$
loss(W) = (Y - W^TX)^2 = (Y - W^TX)^T(Y - W^TX).
$$

损失函数是衡量预测值与真实值之间的差异的，求解计算损失函数的最小值,为了计算方便 我们再在前面乘上一个$\frac12$也不影响计算结果$\frac12(Y - W^TX)^T(Y - W^TX)$。

如果想让误差的值最小，通过对$W$求导，再令导数为 $0$（到达最小极值）
$$
\frac{\partial}{\partial w} loss(W) = X^T(XW-Y) = 0
$$
整理得：
$$
W = (X^TX)^{-1}X^TY
$$

这样就相当于我们直接得到了求解$W$得解析解。


## 4、梯度下降法

see <a href="/#/machine-learning/optimizers/common_optimizers?id=梯度下降法gradient-descent" target="_blank">梯度下降法</a> 


## 5、小结

**面试题**
**1、线性回归要求因变量符合正态分布？**

答：是的。线性回归的假设前提是特征与预测值呈线性关系，误差项符合高斯-马尔科夫条件（零均值，零方差，不相关），这时候线性回归是无偏估计。噪声符合正态分布，那么因变量也符合分布。在进行线性回归之前，要求因变量近似符合正态分布，否则线性回归效果不佳（有偏估计）

**2、如何判断是否符合正态分布？如果不符合正态分布要怎么办？**

答：通过计算数据分布的偏度和峰度值，如果偏度大于3，那么需要对数据进行转换。具体的转换公式需要结合数据分布进行决策。常见的转换方法有：Log转换，根号转换等。

**3、线性回归的前提假设有哪些？**

* 自变量和因变量呈线性关系。
* 误差之间相互独立
* 自变量相互独立
* 误差项的方差应为常数
* 误差呈正态分布

**4、如果线性回归模型效果不好，原因是什么？**

* 自变量和因变量不是线性关系
* 自变量之间不是相互独立
* 模型过拟合。








​