# 线性回归知识

我们在<a href="/#/machine-learning/ml_summay.md" target="_blank">机器学习简介</a>里，用了线性回归的例子来梳理了机器学习大致的流程，
接下来则更详细、更完整的记录**线性回归**的知识。

线性回归是最简单的监督机器学习算法之一，但是麻雀虽小五脏俱全，从线性回归开始机器学习算法的学习是一个不错的选择。

## 一、问题描述
我们说机器学习的任务是去帮我们寻找“规则”，那么线性回归就是假定数据与结果之间的规则是线性关系。

假设我们有特征记为$x_1, x_2, ..., x_n$总共$n$个特征， 预测值记为 $y$。

那么他们之间的关系可以表示为： 

$
y = f(x|w_1, w_2, ... , w_n, b) = w_1*x_1 + w_2*x_2 + ... + w_n*x_n + b
$

再写成向量形式的话：

$
y = f(x|w, b) = w^Tx+b
$

这里的 $w = (w_1; w_2; ...; w_n)$, $x = (x_1; x_2; ...; x_n)$。

注意这里的$w$和$x$均是列向量。

**我们的任务是去确定(学习)到$w$和$b$。**

## 二、损失函数

我们在<a href="/#/machine-learning/loss_func/common_loss_func.md" target="_blank">常见的损失函数</a>中详细介绍了一些常见的损失函数。
线性回归问题中，我们常用的一个损失函数是均方误差(Mean Square Error, MSE)

$$
loss(w, b) = \frac1n \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$
写成向量形式为:
$$
loss(W, b)  = (Y - \hat{Y})^2
$$

这里的$\hat{y}$和$\hat{Y}$是模型的预测值。

损失函数衡量的是真实值与预测值的差异，那我们的问题就可以转换为： 学习到$w$和$b$使得损失函数$MSE$的值最小的最优化问题， 即：

$$
\begin{aligned}

&\ \arg\min_{w, b} loss(w, b)  \\
&= \arg\min_{w, b} \frac1n \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 \\
&= \arg\min_{w, b} \frac1n \sum_{i=1}^{n}[y_i - (wx_i + b)]^2

\end{aligned}
$$

写成向量形式为:
$$
\arg\min_{W, b} [Y - (W^TX+b)]^2
$$


## 三、最小二乘法

我在《数值分析》和《数理统计》这两门课中都学到了这部分内容。《数理统计》好像称为最小二乘估计。

线性回归里面用到的最小二乘法指的是线性最小二乘，当然就有非线性最小二乘法了，我们在后面视情况表出。

进入正题：

为了使得我们的损失函数更加简洁，我们可以假设有一个特征$x_0$恒为$1$，那么我们再使用一个$w_0*x_0$去替代损失函数中的$b$,那么我们的损失函数就变成了
$$
loss(W) = (Y - W^TX)^2 = (Y - W^TX)^T(Y - W^TX).
$$

损失函数是衡量预测值与真实值之间的差异的，所以为了计算方便 我们再在前面乘上一个$\frac12$也不影响计算结果$\frac12(Y - W^TX)^T(Y - W^TX)$。

如果想让误差的值最小，通过对$W$求导，再令导数为 $0$（到达最小极值）
$$
\frac{\partial}{\partial w} loss(W) = X^T(XW-Y) = 0
$$
整理得：
$$
W = (X^TX)^{-1}X^TY
$$

这样就相当于我们直接得到了求解$W$得解析解。


## 四、梯度下降法

see <a href="/#/machine-learning/optimizers/common_optimizers?id=梯度下降法gradient-descent" target="_blank">梯度下降法</a> 


## 五、小结





​